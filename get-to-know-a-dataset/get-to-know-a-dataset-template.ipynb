{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "\n",
    "1. Provide the name of your dataset, replacing the bracketed placeholder text.\n",
    "2. Update the Registry of Open Data landing page URL, by replacing the bracketed placeholder text. The [REGISTRY_YAML_NAME] will correspond to the name of the YAML document in your pull request to the Registry of Open Data on Github, minus the .yaml file extension.\n",
    "3. Remove these comment blocks when you have completed each section.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "# Get to Know a Dataset: [NAME OF DATASET]\n",
    "\n",
    "This notebook serves as a guided tour of the [[NAME OF DATASET - IN THIS TEMPLATE WE WILL USE THE LAST MILE DATASET EXAMPLE FROM AMAZON]](https://registry.opendata.aws/[REGISTRY_YAML_NAME]) dataset. More usage examples, tutorials, and documentation for this dataset and others can be found at the [Registry of Open Data on AWS](https://registry.opendata.aws/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779654-eeee-4708-83cf-245e03303475",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "\n",
    "The goal of this section is to orient users to the structure of your dataset. \n",
    "\n",
    "1. How are key prefixes and objects organized in your S3 bucket?\n",
    "2. What kinds of filetypes are represented in your dataset?\n",
    "3. Explain with text what users are expected to encounter, and then demonstrate with code the organizational framework you applied when creating your dataset.\n",
    "4. The responses to each question section are meant to be expanded or replaced as dictated by your dataset\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: How have you organized your dataset? Help us understand the key prefix structure of your S3 bucket.\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n",
    "At the top level of our S3 bucket, we have a single key prefix \"almrrc2021\" that in turn contains:\n",
    "\n",
    " 1. The dataset license (License.txt)\n",
    " 2. A Readme.txt document\n",
    " 3. Two key prefixes, \"almrrc2021-data-training\" and \"almrrc2021-data-evaluation\" that respectively contain training and testing data in JSON format\n",
    " 4. These in turn contain prefixes corresponding to [...]\n",
    " \n",
    " Full documentation for this dataset can be found at: https://pubsonline.informs.org/doi/10.1287/trsc.2022.1173\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd7832-0c47-4890-a597-de29ca73d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODING GUIDELINES FOR DATA PROVIDER\n",
    "#\n",
    "# General notebook coding guidelines:\n",
    "# 1. Assume that your reader understands the basics of Jupyter Notebooks, Python, and their Python environment.\n",
    "#    The focus of this tutorial is on your dataset.\n",
    "# 2. For library requirements, list the required libraries in a comment block in \"requirements.txt\" format\n",
    "#    (https://pip.pypa.io/en/stable/reference/requirements-file-format/)\n",
    "# 3. Demonstrate importing libraries with the assumption that the user has correctly installed the required\n",
    "#    libraries.\n",
    "# 4. List and load all library dependencies once, at this point of the notebook, unless a complicated dependency\n",
    "#    set makes it unweildy.\n",
    "# 5. Remember, the goal of this tutorial is a 101-level introduction to your dataset using common tools and libraries.\n",
    "#    Examples using specialized environments and deep-diving methods are better suited to follow-up tutorials.\n",
    "#\n",
    "# CODING GUIDELINES FOR DATA PROVIDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b47b69",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "First we will import the Python libraries required throughout this notebook.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "\n",
    "# This notebook requires the following additional libraries\n",
    "# (please install using the preferred method for your environment, e.g. pip, conda):\n",
    "#\n",
    "# boto3 >= 1.38.23\n",
    "# polars >= 1.30.0\n",
    "# matplotlib >= 3.10.3 \n",
    "\n",
    "# Import the libraries required for this notebook\n",
    "# Built-ins\n",
    "import json\n",
    "from pprint import pprint\n",
    "# Installed libraries\n",
    "import boto3, polars, matplotlib.pyplot as plt\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ae10",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Next, we will define the location of our dataset, create our boto3 S3 client, and list the top level prefixes in our S3 bucket. Here we see there is only one top-level prefix in our bucket.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Location of the S3 bucket for this dataset\n",
    "bucket = \"amazon-last-mile-challenges\"\n",
    "\n",
    "# List the top level of the bucket using boto3. Because this is a public bucket, we don't need to sign requests.\n",
    "# Here we set the signature version to unsigned, which is required for public buckets.\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# Print the items in the top-level prefixes\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Delimiter='/')['CommonPrefixes']:\n",
    "    print(item['Prefix'])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9fa4d",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Looking into the top-level S3 prefix of our dataset, we see that the data have been separated into training and evaluation datasets.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# List the key prefixes within the top level 'almrrc2021' prefix\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix='almrrc2021/', Delimiter='/', MaxKeys=10)['CommonPrefixes']:\n",
    "    print(item['Prefix'])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47268012",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "The training and evaluation prefixes are similar in structure, and so we can look into the training portion to get a sense of the deeper structure of the dataset where the data objects reside.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b40a5-67f0-4246-a49c-8c04d4efacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# List the keys within the 'almrrc2021/almrrc2021-data-training' prefix.\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix='almrrc2021/almrrc2021-data-training/', MaxKeys=100)['Contents']:\n",
    "    print(item['Key'])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4bcf-ec40-432f-a31f-4477efa205ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is meant to orient users of your dataset to the formats present in your dataset, particularly if your dataset includes formats that may be unfamiliar to a general data scientist audience. This section should include:\n",
    "\n",
    "1. Explanation of data format(s) (very common formats can be very briefly described, while less common\n",
    "   or domain specific formats should include more explanation as well as links to official documentation)\n",
    "2. Explanation of why the data format was chosen for your dataset\n",
    "3. Recommendations around software and tooling to work with this data format\n",
    "4. Explanation of any dataset-specific aspects to your usage of the format\n",
    "5. Description of AWS services that may be useful to users working with your data\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What data formats are present in your dataset? What kinds of data are stored using these formats? Can you give any advice for how you work with these data formats?\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n",
    "Our dataset comes as a set of JSON (JavaScript Object Notation) files organized as shown in the last section. Generically, JSON is a lightweight, text-based data format that uses key-value pairs and arrays to structure data in a human-readable and machine-parseable way. It supports nested structures and primarily works with:\n",
    "\n",
    "- Objects (key-value pairs)\n",
    "- Arrays\n",
    "- Basic data types (strings, numbers, booleans, null)\n",
    "\n",
    "Our dataset used this format because JSON:\n",
    " - naturally represents delivery route data's hierarchical nature (routes containing stops containing packages)\n",
    " - handles mixed data types well (coordinates, timestamps, delivery constraints)\n",
    " - is easily processed by most programming languages\n",
    " - is human-readable format helps with data validation and debugging\n",
    " - efficiently represents sparse data in cases where not all entries have the same fields\n",
    "\n",
    "These JSON files contain data contain route, stop, and package level features as fully documented in [this journal article](https://pubsonline.informs.org/doi/10.1287/trsc.2022.1173).\n",
    "\n",
    "JSON is well supported by Python through its built in 'json' library. Packages such as Pandas and Polars can be used to work with JSON as well. Lastly, services such as [Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/querying-JSON.html), and [Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/dg/json-functions.html) can be used to query JSON data at scale using SQL.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362bd15",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal of this section is to demonstrate loading a portion of data from your dataset, and reveal something about its structure.\n",
    "1. Load an object from S3\n",
    "2. Show the structure of data in the object\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: Can you show us an example of downloading and loading data from your dataset?\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n",
    "As an example, let us load up and look into some package data as found in s3://amazon-last-mile-challenges/almrrc2021/almrrc2021-data-training/model_apply_inputs/new_package_data.json.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# First we'll load the data into a Python dictionary using the built-in json library\n",
    "\n",
    "file_key = \"almrrc2021/almrrc2021-data-training/model_build_inputs/package_data.json\"\n",
    "\n",
    "with s3.get_object(Bucket=bucket, Key=file_key)['Body'] as file_object:\n",
    "    package_data = json.load(file_object)\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686af22a",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "First, let's take a look at the keys in our newly-loaded dataset. Here we see that top-level keys correspond to route IDs.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# pretty print a truncated list of keys in our dictionary\n",
    "pprint(list(package_data.keys())[:10])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9062e1",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Next we'll look at packages associated with the first route ID in our dataset to get a sense for the structure of this file. Here we note that each package ID in this file has dimensions as well as a planned service time duration.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7209334",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# pretty print the structure an individual route record\n",
    "pprint(package_data[\"RouteID_15baae2d-bf07-4967-956a-173d4036613f\"])\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717987e-c9c9-4c0f-9f2e-a8d761492625",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "The goal here is to visualize some aspect of your dataset in order to help users understand it. In addition to helping users of your dataset understand the dataset, an additional goal is to impress!\n",
    "\n",
    "Please demonstrate any data preprocessing or reshaping required for your visualization(s).\n",
    "\n",
    "https://www.reddit.com/r/dataisbeautiful/ for inspiration.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: A picture is worth a thousand words. Show us a visual (or several!) from your dataset that either illustrates something informative about your dataset, or that you think might excite someone to dig in further.\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n",
    "For this example, we will first demonstrate the distribution of package sizes (cm^3). To aid ourselves, we can first flatten the nested dictionary structure we created in the last section.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Create a function to flatten our data structure\n",
    "def flatten_package_data(data):\n",
    "    flattened = []\n",
    "    \n",
    "    for route_key, route_data in data.items():\n",
    "        route_id = route_key.split('_')[1]  # Extract RouteID part\n",
    "        \n",
    "        # Iterate through all zone dictionaries (like 'AH')\n",
    "        for zone_data in route_data.values():\n",
    "            # Iterate through package dictionaries\n",
    "            for package_key, package_info in zone_data.items():\n",
    "                package_id = package_key.split('_')[1]  # Extract PackageID part\n",
    "                \n",
    "                flattened.append({\n",
    "                    'RouteID': route_id,\n",
    "                    'PackageID': package_id,\n",
    "                    'depth_cm': package_info['dimensions']['depth_cm'],\n",
    "                    'height_cm': package_info['dimensions']['height_cm'],\n",
    "                    'width_cm': package_info['dimensions']['width_cm'],\n",
    "                    'planned_service_time_seconds': package_info['planned_service_time_seconds']\n",
    "                })\n",
    "    \n",
    "    return polars.DataFrame(flattened)\n",
    "\n",
    "# Convert to Polars DataFrame\n",
    "df = flatten_package_data(package_data)\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5cdf3",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now let's take a look at the first few records of our newly flattened dataset.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef33976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Print first few rows of our newly flattened dataset\n",
    "df.head()\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd9347",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now let's add another column, volume_cm3, that gives us the volume of each package.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Calculate volume by multiplying length, width, and depth dimensions\n",
    "df = df.with_columns(\n",
    "    (polars.col('depth_cm') * polars.col('height_cm') * polars.col('width_cm')).alias('volume_cm3')\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303cbc82",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now we can plot the distribution of package volumes for the entirety of our dataset.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df75c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "\n",
    "# Plot using matplotlib\n",
    "# Set figure size and DPI for better resolution\n",
    "plt.figure(figsize=(12, 7), dpi=100, facecolor='white')\n",
    "\n",
    "# Create histogram with custom styling\n",
    "plt.hist(df['volume_cm3'], \n",
    "         bins=30,\n",
    "         color='#3498db',    # Nice blue color\n",
    "         edgecolor='white',\n",
    "         linewidth=1.2,\n",
    "         alpha=0.8)\n",
    "\n",
    "# Customize title and labels\n",
    "plt.title('Distribution of Package Volumes', \n",
    "         fontsize=16, \n",
    "         pad=20, \n",
    "         fontweight='bold')\n",
    "plt.xlabel('Volume (cmÂ³)', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Count', fontsize=12, labelpad=10)\n",
    "\n",
    "# Add grid with custom styling\n",
    "plt.grid(True, linestyle='--', alpha=0.3, color='gray')\n",
    "\n",
    "# Customize axes and background\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#f8f9fa')  # Light gray background\n",
    "ax.spines['top'].set_visible(False)    # Remove top border\n",
    "ax.spines['right'].set_visible(False)  # Remove right border\n",
    "ax.spines['left'].set_linewidth(0.5)   # Thin left border\n",
    "ax.spines['bottom'].set_linewidth(0.5) # Thin bottom border\n",
    "\n",
    "# Adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2f5a5",
   "metadata": {},
   "source": [
    "EXAMPLE - REPLACE\n",
    "\n",
    "Now, let's take a look and see if we can get a sense of how many packages there are to deliver per route.\n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE - REPLACE\n",
    "\n",
    "# Group by RouteID and count PackageID\n",
    "packages_per_route = df.group_by('RouteID').agg(\n",
    "    polars.col('PackageID').count()\n",
    ").get_column('PackageID')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 7), dpi=100, facecolor='white')\n",
    "\n",
    "plt.hist(packages_per_route, \n",
    "         bins=30,\n",
    "         color='#3498db',\n",
    "         edgecolor='white',\n",
    "         linewidth=1.2,\n",
    "         alpha=0.8)\n",
    "\n",
    "plt.title('Distribution of Packages per Route', \n",
    "         fontsize=16, \n",
    "         pad=20, \n",
    "         fontweight='bold')\n",
    "plt.xlabel('Number of Packages', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Number of Routes', fontsize=12, labelpad=10)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3, color='gray')\n",
    "\n",
    "# Customize axes and background\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics using Polars\n",
    "print(f\"Average packages per route: {packages_per_route.mean():.1f}\")\n",
    "print(f\"Median packages per route: {packages_per_route.median():.1f}\")\n",
    "print(f\"Min packages per route: {packages_per_route.min()}\")\n",
    "print(f\"Max packages per route: {packages_per_route.max()}\")\n",
    "\n",
    "### EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c8b85-ed1c-4f2c-bd0e-fbfbc67c4723",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is less prescriptive / freeform than previous sections. The goal here is to show an opinionated example of answering a question using your data. The scale of your dataset may preclude a full example, and so feel free to limit the scope of this example (e.g. work on a subset of data). Users should be able to replicate your example in this notebook, and get a sense of how they would scale up.\n",
    "\n",
    "A \"toy\" example is better than no example.\n",
    "\n",
    "Ideally, your example would:\n",
    "1. Transmit some of your domain & dataset experience to the reader, drawing on your own work as much as possible\n",
    "2. Provide a jumping off point for users to extend your work, and do novel work of their own.\n",
    "\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one question that you have answered using these data? Can you show us how you came to that answer?\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent sem enim, finibus vel leo vel, blandit interdum tortor. In lectus dolor, congue eu odio vel, euismod facilisis lacus. Vestibulum aliquet, quam in rhoncus tempus, arcu massa suscipit lacus, nec fermentum quam justo nec lacus. Duis id leo fermentum ante tempor pulvinar eu vitae ligula. Cras feugiat vel ligula sit amet lacinia. Mauris sit amet sem vestibulum ligula volutpat iaculis in eu velit. Sed turpis magna, porta ac nisi vitae, maximus volutpat mi. Vestibulum mattis est eros, nec pellentesque nisi iaculis sed. \n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf645724-3108-4ada-a832-10b3431eb8e2",
   "metadata": {},
   "source": [
    "<!-- DATA PROVIDER INSTRUCTIONS\n",
    "This section is, like the previous one, intended to be freeform / non-prescriptive. The goal here is to provide a challenge to the community to do something novel with your dataset. That can either be novel in terms of the task, or novel in terms of methodological or computational approach.\n",
    "\n",
    "Another way to consider this section, is as a wishlist. If you were less constrained by time, cost, skill, etc., what would you like to see achieved using these data? \n",
    "\n",
    "The challenge should, however, be somewhat realistic. A challenge that assumes e.g. original data collection, is likely to go unanswered.\n",
    "DATA PROVIDER INSTRUCTIONS -->\n",
    "\n",
    "### Q: What is one unanswered question that you think could be answered using these data? Do you have any recommendations or advice for someone wanting to answer this question?\n",
    "\n",
    "EXAMPLE - REPLACE\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent sem enim, finibus vel leo vel, blandit interdum tortor. In lectus dolor, congue eu odio vel, euismod facilisis lacus. Vestibulum aliquet, quam in rhoncus tempus, arcu massa suscipit lacus, nec fermentum quam justo nec lacus. Duis id leo fermentum ante tempor pulvinar eu vitae ligula. Cras feugiat vel ligula sit amet lacinia. Mauris sit amet sem vestibulum ligula volutpat iaculis in eu velit. Sed turpis magna, porta ac nisi vitae, maximus volutpat mi. Vestibulum mattis est eros, nec pellentesque nisi iaculis sed. \n",
    "\n",
    "EXAMPLE - REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ca021",
   "metadata": {},
   "source": [
    "# DATA PROVIDER: PLEASE REMEMBER TO CLEAR ALL OUTPUTS BEFORE COMMITTING TO YOUR GITHUB REPOSITORY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
